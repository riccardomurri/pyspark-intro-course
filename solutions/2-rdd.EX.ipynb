{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a><div align=\"center\">This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</div>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we would need to initialize a Spark context in order to perform initialization.  However, this is automatically done in this IPython installation, so we skip this part. (Only *one* SparkContext object can be alive at the same time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SparkContext(appName=\"PythonPi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the following cell to confirm that the PySpark kernel has been started correctly and that a SparkContext is already available in variable `sc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.23.111.173:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.A\n",
    "\n",
    "*How many lines are there in text file `hdfs:///shakespeare.txt.gz?`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us read the entire (compressed) text file into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read (compressed) text file\n",
    "rdd = sc.textFile('hdfs:///shakespeare.txt.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that each element in the RDD is a single line of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The Project Gutenberg EBook of The Complete Works of William Shakespeare, by',\n",
       " u'William Shakespeare',\n",
       " u'',\n",
       " u'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " u'almost no restrictions whatsoever.  You may copy it, give it away or']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can just print the count of elements in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124787\n"
     ]
    }
   ],
   "source": [
    "# each element in the RDD is a line; print their count\n",
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.B\n",
    "\n",
    "*Compute the product of the sequence of numbers `[1, 2, 3, 4, 5]` using PySpark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the sequence into an RDD\n",
    "rdd = sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for multiplying two numbers\n",
    "def mul(x,y):\n",
    "    return (x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the `mul` function to reduce the RDD to one single value\n",
    "result = rdd.reduce(mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your solution is correct, evaluating the following cell should print `120`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.C\n",
    "\n",
    "*A very simple technique for checksumming a stream of bytes is [computing the “bit parity”](https://en.wikipedia.org/wiki/Checksum#Parity_byte_or_parity_word) (i.e., number of “1” bits) of each bit position in a byte.*\n",
    "\n",
    "*Compute the bit parity checksum of the above ASCII text file using PySpark.*\n",
    "\n",
    "**Hint:** *you can compute parity of two bytes `a` and `b` by\n",
    "combining them with Python’s `a^b` operator (XOR); it is\n",
    "a commutative and associative operation. See [here](https://en.wikipedia.org/wiki/Checksum#Parity_byte_or_parity_word) for a more detailed description.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us read the entire text file into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('hdfs:///shakespeare.txt.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to reduce the entire RDD to a single value -- this calls for a folding operation, i.e., `reduce`, `fold`, or `aggregate`.\n",
    "\n",
    "Now, the issue is that each element of the RDD is a string of characters (corresponding to a line of text), whereas we want to compute a byte-based XOR.  However, Python too has a built-in `reduce()` function.  Since bit parity is additive:\n",
    "\n",
    "1. a function for computing the checksum of a single line\n",
    "2. a function for updating the combined checksum of a group of lines, given an additional line (uses 1.)\n",
    "3. a function to combine two checksums of different groups of lines.\n",
    "\n",
    "So the right \"folding\" variant to use is `aggregate` with operations 2. and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. a function for computing the checksum of a single line\n",
    "\n",
    "def checksum(line):\n",
    "    \"Compute XOR checksum of a line of text\"\n",
    "    return reduce(upd_char_checksum, line, 0)\n",
    "\n",
    "def upd_char_checksum(val, ch):\n",
    "    \"Update checksum `val` by XOR-ing with ASCII code of character `ch`\"\n",
    "    return (val ^ ord(ch))\n",
    "\n",
    "# example\n",
    "checksum('bit parity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. a function for updating the checksum of a group of lines, given one additional line\n",
    "\n",
    "def update_checksum(val, line):\n",
    "    return (val ^ checksum(line))\n",
    "\n",
    "# example\n",
    "update_checksum(88, 'bit parity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. a function to combine two checksums\n",
    "\n",
    "def combine_checksums(val1, val2):\n",
    "    return (val1 ^ val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 2.8 s\n",
      "The XOR checksum is: 10\n"
     ]
    }
   ],
   "source": [
    "# finally, use PySpark's `aggregate()`\n",
    "\n",
    "%time cksum = rdd.aggregate(0, update_checksum, combine_checksums)\n",
    "\n",
    "print(\"The XOR checksum is: %d\" % cksum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.D\n",
    "\n",
    "*Implement a the “word count” algorithm using PySpark, and use it to count the words in file `hdfs:///shakespeare.txt.gz`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “word count” algorithm is comprised of the following steps:\n",
    "\n",
    "1. Read lines of a text file into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. read lines of text file into RDD\n",
    "rdd1 = sc.textFile('hdfs:///shakespeare.txt.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Transform the RDD by splitting each line at blank spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(line):\n",
    "    \"Split a line of text into words.\"\n",
    "    return (line\n",
    "           .lower()\n",
    "           .split())\n",
    "    \n",
    "rdd2 = rdd1.flatMap(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the', u'project', u'gutenberg', u'ebook', u'of']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a key/value RDD by pairing each word with the value 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_pair(word):\n",
    "    \"Map a single `word` into a *(word, 1)* pair\"\n",
    "    return (word, 1)\n",
    "\n",
    "rdd3 = rdd2.map(word_to_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 1), (u'project', 1), (u'gutenberg', 1), (u'ebook', 1), (u'of', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sum the values associated with each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "rdd4 = rdd3.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'fawn', 11),\n",
       " (u'considered,', 2),\n",
       " (u'considered.', 3),\n",
       " (u'mustachio', 1),\n",
       " (u'fleeces', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can collect the result on the driver using `rdd4.collectAsMap()` or keep the RDD around for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What are the 5 most frequent words?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either sort `rdd4` and then take the first 5 elements, or use `takeOrdered()` which combines the two operation into one.  However, documentation of the latter method warns us that all sorting is done in the Spark driver memory, so it is only suitable for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nr_of_occurrences(pair):\n",
    "    \"Return nr. of occurrences from *(word, count)* pair.\"\n",
    "    word, count = pair\n",
    "    return count\n",
    "\n",
    "rdd5 = rdd4.sortBy(nr_of_occurrences, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 27730),\n",
       " (u'and', 26099),\n",
       " (u'i', 19540),\n",
       " (u'to', 18762),\n",
       " (u'of', 18126)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.E\n",
    "\n",
    "*Compute an approximation of the mathematical constant $\\pi$ using the Monte Carlo method of estimating the size of the unit circle.  (See exercise 1.E for more details and a pure-Python solution.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since also Spark RRDs provide `map` and `filter` methods, we can straightforwardly translate the Python solution from *Exercise 1.E*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the number of iterations; since the algorithm converges very slowly, we want this number to be large:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A constant that is unique to Spark is the number of partitions, which controls the gramularity of parallelism (more partitions, smaller chunks of work sent to the executors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = sc.defaultParallelism*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process data in Spark, we have to inject it in the system using `SparkContext.parallelize()` and get an RDD back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# inject an array of size N into Spark\n",
    "dummy = sc.parallelize(xrange(N), partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs provide a `.map()` method that creates a new RDD by applying the given function elementwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 37 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "def point(_):\n",
    "    \"Return coordinates of a random point in the unit square.\"\n",
    "    x = random.random()\n",
    "    y = random.random()\n",
    "    return (x,y)\n",
    "\n",
    "# create collection of random points\n",
    "points = dummy.map(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the `.filter()` method creates a new RDD by copying only those values on which the supplied predicate is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 57.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def in_unit_circle(coords):\n",
    "    x, y = coords\n",
    "    return (x**2 + y**2) < 1\n",
    "\n",
    "# keep only those in unit circle\n",
    "points_in_unit_circle = points.filter(in_unit_circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `.count()` method returns the number of elements in a RDD (like Python's built-in `len()`).  \n",
    "\n",
    "Note that `.count()` is an **action** so it triggers actual computation of the DAG we have been building so far -- almost all the work that Spark does is concentrated in this single line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 12 ms, total: 12 ms\n",
      "Wall time: 36.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# count the latter\n",
    "P = points_in_unit_circle.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.142004\n"
     ]
    }
   ],
   "source": [
    "pi = 4.0 * P / N\n",
    "\n",
    "print(\"Pi is roughly %f\" % pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative solution\n",
    "\n",
    "This algorithm can be realized via a Map/Reduce scheme by using mappers that take no input and output 0 or 1 depending on whether a single randomly-generated point landed inside the circle or not.  Then the reducers simply add all the results.\n",
    "\n",
    "This solution does not strictly depend on Spark-only features and could in principle run on any Map/Reduce engine.  On the other hand, all the worker is done on the mappers and the reduce step does not parallelize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two parameters `partitions` and `n` control the number of mappers and the number of repetitions that Spark is going to execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 16\n",
    "n = 1000000 * partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper function just generates a random point in the square [0, +1] x [0, +1] and then outputs 1 if it fell inside the unit circle and 0 if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomly_in_unit_circle(_):\n",
    "    x = random.random()\n",
    "    y = random.random()\n",
    "    if (x**2 + y**2) < 1:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *reduce* function just adds all these 0's and 1's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a,b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the total number of positive samples is found by applying map/reduce to a list of length `n`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count = (sc.parallelize(xrange(N), partitions)\n",
    "         .map(randomly_in_unit_circle)\n",
    "         .reduce(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the area of the circle is computed as the number of positive samples divided by the area of the enclosing square (all samples land in the square); the factor `4` comes from the fact that we're sampling only the x>0, y>0 sector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 19.639090\n"
     ]
    }
   ],
   "source": [
    "pi = 4.0 * count / n\n",
    "\n",
    "print(\"Pi is roughly %f\" % pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally the SparkContext is stopped when the computations are done.  We don't do it here in order to be able to run more computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
