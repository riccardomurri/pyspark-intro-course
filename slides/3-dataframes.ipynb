{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a><div align=\"center\">This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Normally, we would need to initialize a [`SparkContext`](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-SparkContext.html#creating-instance) in order to perform initialization.  However, this is automatically done in this IPython installation, so we skip this part. (Only *one* `SparkContext` object can be alive at the same time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#sc = SparkContext(appName=\"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Evaluate the following cell to confirm that the PySpark kernel has been started correctly and that a `SparkContext` is already available in variable `sc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.23.111.173:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is the DataFrame API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new API for computing with structured data\n",
    "(if it’s a table, it fits).\n",
    "\n",
    "* Data in a DataFrame follows a specified “schema” (what type of data is in each column)\n",
    "* Can give some nice performance improvements:\n",
    "  - Spark can optimize the execution because it knows more about the data layout\n",
    "  - most of the work can take place in the JVM without need to ship back and forth to Python\n",
    "* Load and save data in a variety of formats, e.g. JSON, Parquet, SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating DataFrames\n",
    "\n",
    "The entry point for the DataFrame API is called a [`SparkSession`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession).\n",
    "\n",
    "A `SparkSession` must be created from an active `SparkContext`:\n",
    "\n",
    "```\n",
    "# ‘sc‘ is the SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We are going to use public data from the [Wordbank project](http://wordbank.stanford.edu/) for examples in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PySpark's `spark.read.csv()` function to read a CSV file into a DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"hdfs:///wordbank.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Method `.show()` on the DataFrame can be used to show the first few rows (by default 20):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----------+---+--------+------+-----------+---------+--------+---------+\n",
      "| _c0|    _c1|        _c2|_c3|     _c4|   _c5|        _c6|      _c7|     _c8|      _c9|\n",
      "+----+-------+-----------+---+--------+------+-----------+---------+--------+---------+\n",
      "|null|data_id|num_item_id|age|language|   sex|birth_order|ethnicity|produces|     word|\n",
      "|   0|  51699|         13| 27| English|Female|     Fourth| Hispanic|   False|alligator|\n",
      "|   1|  51699|         14| 27| English|Female|     Fourth| Hispanic|    True|   animal|\n",
      "|   2|  51699|         15| 27| English|Female|     Fourth| Hispanic|   False|      ant|\n",
      "|   3|  51699|         16| 27| English|Female|     Fourth| Hispanic|    True|     bear|\n",
      "|   4|  51699|         17| 27| English|Female|     Fourth| Hispanic|   False|      bee|\n",
      "|   5|  51699|         18| 27| English|Female|     Fourth| Hispanic|    True|     bird|\n",
      "|   6|  51699|         19| 27| English|Female|     Fourth| Hispanic|    True|      bug|\n",
      "|   7|  51699|         20| 27| English|Female|     Fourth| Hispanic|    True|    bunny|\n",
      "|   8|  51699|         21| 27| English|Female|     Fourth| Hispanic|   False|butterfly|\n",
      "+----+-------+-----------+---+--------+------+-----------+---------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other methods for reading data into a Spark DataFrame are:\n",
    "\n",
    "* [`spark.read.json()`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json): read data from a [JSON](https://www.json.org/) file\n",
    "* [`spark.read.jdbc()`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.jdbc): read data from an SQL database\n",
    "* [`spark.read.parquet()`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.parquet): read data from a [Parquet](https://parquet.apache.org/)-format directory\n",
    "* [`spark.read.text()`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.text): read a text file; the resulting DataFrame has a single column `value`, and a row for each line of the input text.\n",
    "\n",
    "All the above methods (except `read.jdbc`) accept a `hdfs://`, `file://` or any other URI format accepted by Spark, and can automatically decompress files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Like in SQL, a DataFrame schema is a mapping *columns → column value type*.\n",
    "\n",
    "Unlike SQL, Spark’s DataFrame objects allow composite columns (i.e., columns whose value is a list or tuple), so a the schema is a hierachical / tree-like structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the ‘.printSchema()’ method of a DataFrame\n",
    "object to print a human-readable representation of its\n",
    "schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 3.A\n",
    "\n",
    "Read the documentation for the [`spark.read.csv`]() function and find out how to modify the above function call so that: *(1)* column names are taken from the first row (header) of the file; *(2)* column types are inferred from the data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"hdfs:///wordbank.csv.gz\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If your way of reading data was right, evaluating the following cells should present a table with columnes `_c0`, `data_id`, `num_item_id`, `age`, ..., `produces`, `word`, of which the `_c0`, `data_id`, and `num_item_id` are integers, `produces` is boolean, and the rest are string-valued:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+---+--------+------+-----------+---------+--------+---------+\n",
      "|_c0|data_id|num_item_id|age|language|   sex|birth_order|ethnicity|produces|     word|\n",
      "+---+-------+-----------+---+--------+------+-----------+---------+--------+---------+\n",
      "|  0|  51699|         13| 27| English|Female|     Fourth| Hispanic|   false|alligator|\n",
      "|  1|  51699|         14| 27| English|Female|     Fourth| Hispanic|    true|   animal|\n",
      "|  2|  51699|         15| 27| English|Female|     Fourth| Hispanic|   false|      ant|\n",
      "|  3|  51699|         16| 27| English|Female|     Fourth| Hispanic|    true|     bear|\n",
      "|  4|  51699|         17| 27| English|Female|     Fourth| Hispanic|   false|      bee|\n",
      "+---+-------+-----------+---+--------+------+-----------+---------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look for headers and first values\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- data_id: integer (nullable = true)\n",
      " |-- num_item_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- birth_order: string (nullable = true)\n",
      " |-- ethnicity: string (nullable = true)\n",
      " |-- produces: boolean (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DataFrame objects can be created from an RDD of tuple/list/dict, Python list of the same, or pandas.DataFrame.\n",
    "\n",
    "Unless using dict or pandas.DataFrame items, column names must be given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ppl = spark.createDataFrame(\n",
    "  [('Alice', 1), ('Bob', 2)],\n",
    "  schema=['name', 'id'])\n",
    "\n",
    "ppl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more realistic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|                  x|                sin|               cos|\n",
      "+-------------------+-------------------+------------------+\n",
      "|                0.0|                0.0|               1.0|\n",
      "|                0.1|0.09983341664682815|0.9950041652780258|\n",
      "|                0.2|0.19866933079506122|0.9800665778412416|\n",
      "|0.30000000000000004| 0.2955202066613396| 0.955336489125606|\n",
      "|                0.4| 0.3894183423086505|0.9210609940028851|\n",
      "+-------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. create arrays of values\n",
    "import math\n",
    "xs = [(0.0 + 0.1*i) for i in range(20)]\n",
    "cos_x = [math.cos(x) for x in xs]\n",
    "sin_x = [math.sin(x) for x in xs]\n",
    "\n",
    "# 2. create DataFrame with the given columns\n",
    "trig = spark.createDataFrame(zip(xs, sin_x, cos_x), \n",
    "                             schema=['x', 'sin', 'cos'])\n",
    "\n",
    "# 3. show a few sample columns\n",
    "trig.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To and From PanDas DataFrames\n",
    "\n",
    "[PanDas](http://pandas.pydata.org) provides the \"standard\" DataFrame implementation for Python.  PySpark can create DataFrames from PanDas ones (with `createDataFrame`).  Note that column names and types are automatically taken from PanDas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "xs = range(20)\n",
    "ys = [x*x for x in xs]\n",
    "pandas_df = pd.DataFrame({'x': xs, 'y': ys})\n",
    "\n",
    "spark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  4|\n",
      "+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can convert a Spark DataFrame to a PanDas DF by using method `.toPandas()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x   y\n",
       "0  0   0\n",
       "1  1   1\n",
       "2  2   4\n",
       "3  3   9\n",
       "4  4  16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### To and From RDDs\n",
    "\n",
    "RDDs can be converted into Spark DataFrames; this operation requires specifying types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('Alice', 1), ('Bob', 2)])\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "spark_df = spark.createDataFrame(rdd, schema=StructType([\n",
    "    StructField('name', StringType()),\n",
    "    StructField('id', IntegerType())\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are a few rules to follow here:\n",
    "\n",
    "* `schema=...` is always an instance of `StructType()`\n",
    "* `StructType(...)` contains a *list* of `StructField`*(name, dtype)*\n",
    "* *dtype* is an *instance* of any of the type designators in module [`pyspark.sql.types`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#module-pyspark.sql.types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can grab the RDD underlying *any* DataFrame using the `.rdd` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.B\n",
    "\n",
    "Modify the \"word count\" code of *Exercise 2.D* to return the word counts as a DataFrame instead of an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# assuming we have a `wordcount()` function already defined\n",
    "\n",
    "def wordcount2(source):\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "    rdd = wordcount(source)\n",
    "    df = spark.createDataFrame(rdd, schema=StructType([\n",
    "        StructField('word', StringType()),\n",
    "        StructField('count', IntegerType()),\n",
    "    ]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Table manipulations\n",
    "\n",
    "As with RDDs, Spark DataFrames are **immutable**.\n",
    "\n",
    "So, all transformations of an RDD return a *new* RDD and leave the original one unmodified.\n",
    "\n",
    "(Hence, for example, removing a column does not really remove a column: it just returns a view on the original DF with one column omitted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Removing columns\n",
    "\n",
    "In order to do some data cleaning (and save memory), we drop the columns that are not used in the forthcoming analysis. A column may be deleted from the `DataFrame` with the method `.drop()`.\n",
    "\n",
    "Columns may be removed by giving column names as strings:\n",
    "\n",
    "    df2 = df.drop('_c0', 'num_item_id')\n",
    "    \n",
    "or as [`Column`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.Column) objects:\n",
    "\n",
    "    df2 = df.drop(df._c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---+--------+------+-----------+---------+--------+---------+\n",
      "|data_id|num_item_id|age|language|   sex|birth_order|ethnicity|produces|     word|\n",
      "+-------+-----------+---+--------+------+-----------+---------+--------+---------+\n",
      "|  51699|         13| 27| English|Female|     Fourth| Hispanic|   false|alligator|\n",
      "|  51699|         14| 27| English|Female|     Fourth| Hispanic|    true|   animal|\n",
      "|  51699|         15| 27| English|Female|     Fourth| Hispanic|   false|      ant|\n",
      "|  51699|         16| 27| English|Female|     Fourth| Hispanic|    true|     bear|\n",
      "+-------+-----------+---+--------+------+-----------+---------+--------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.drop('_c0')\n",
    "\n",
    "df2.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Replacing values in columns\n",
    "\n",
    "Method `.replace()` accept a Python dictionary mapping values that should be replaced with their replacements (**note:** \n",
    "they should have the same type!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---+--------+---+-----------+---------+--------+---------+\n",
      "|data_id|num_item_id|age|language|sex|birth_order|ethnicity|produces|     word|\n",
      "+-------+-----------+---+--------+---+-----------+---------+--------+---------+\n",
      "|  51699|         13| 27| English|  1|     Fourth| Hispanic|   false|alligator|\n",
      "|  51699|         14| 27| English|  1|     Fourth| Hispanic|    true|   animal|\n",
      "|  51699|         15| 27| English|  1|     Fourth| Hispanic|   false|      ant|\n",
      "+-------+-----------+---+--------+---+-----------+---------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.replace({'Male':'0', 'Female':'1'}).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can restrict replacement to a subset of the columns with an additional `subset=...` argument:\n",
    "```\n",
    "df3 = df2.replace({'Male':'0', 'Female':'1'}, subset=['sex'])\n",
    "```\n",
    "\n",
    "If there is only one value to replace, a shortcut form is allowed:\n",
    "```\n",
    "df3 = df2.replace('English', 'en', subset=['language'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting a subset of the columns (projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset of the columns may be chosen using the `.select()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|age|language|   sex|\n",
      "+---+--------+------+\n",
      "| 27| English|Female|\n",
      "| 27| English|Female|\n",
      "| 27| English|Female|\n",
      "| 27| English|Female|\n",
      "| 27| English|Female|\n",
      "+---+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(['age', 'language', 'sex'])\n",
    "\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It looks as if we've picked constant columns? A `Column`'s `.describe()` method returns a new DF with a quick statistical summary of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              age|\n",
      "+-------+-----------------+\n",
      "|  count|           234350|\n",
      "|   mean|22.34275229357798|\n",
      "| stddev|4.731628099530239|\n",
      "|    min|               16|\n",
      "|    max|               30|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.describe('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that Spark will happily compute descriptive statistics for non-numeric columns as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|summary|language|\n",
      "+-------+--------+\n",
      "|  count|  234350|\n",
      "|   mean|    null|\n",
      "| stddev|    null|\n",
      "|    min| English|\n",
      "|    max| English|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.describe('language').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other transformations\n",
    "\n",
    "All these methods return a new DF:\n",
    "\n",
    "* `distinct(cols)`:  all unique rows of the source DF\n",
    "* `dropDuplicates(cols)`: return new DF where all in the named columns are distinct; if no columns are given, default to all columns in the source DF.\n",
    "* `orderBy(col)`: new DF sorted according to the given column expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Column expressions\n",
    "\n",
    "The syntax *df.column* (e.g., `df2.age`) can be used to write logical, string, or arithmetic expressions involving column values.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* `df.sex == 'Male'`\n",
    "* `df.age * 12`\n",
    "\n",
    "Such expressions expand to a new value for every row when used in `.select()` or `.where()`/`.filter()`.\n",
    "\n",
    "A column expression may be suffixed with `.alias('name')` to give it a name in the resulting DF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Create new DF with boolean column for age less/greater than 2 yrs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+\n",
      "|language|   sex|(age > 24)|\n",
      "+--------+------+----------+\n",
      "| English|Female|      true|\n",
      "| English|Female|      true|\n",
      "| English|Female|      true|\n",
      "+--------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.select(df.language, 'sex', df.age > 24)\n",
    "\n",
    "df3.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Create new DF with an additional column giving age in years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+\n",
      "|language|   sex|age_yrs|\n",
      "+--------+------+-------+\n",
      "| English|Female|      2|\n",
      "| English|Female|      2|\n",
      "| English|Female|      2|\n",
      "+--------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df2.select('language', 'sex', (df.age / 12).cast('integer').alias('age_yrs'))\n",
    "\n",
    "df4.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selection by rows\n",
    "\n",
    "Boolean column expressions can be used with `.filter()` (also known as `.where()`) to create a new DF having only rows that match a predicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "|age|language| sex|\n",
      "+---+--------+----+\n",
      "| 27| English|Male|\n",
      "| 27| English|Male|\n",
      "| 27| English|Male|\n",
      "+---+--------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df2.where(df.sex=='Male')\n",
    "\n",
    "df5.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Exercise 3.C\n",
    "\n",
    "Count the number of rows where column `'sex'` has the value `'Male'` and the count of rows where column `sex` has value `'Female'`.  Does this equal the total number of rows in the dataset?  If not, what other values for `sex` are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89053"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df.sex == 'Male').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83979"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df.sex == 'Female').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() == df.where(df.sex == 'Male').count() + df.where(df.sex == 'Female').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   sex|\n",
      "+------+\n",
      "|  null|\n",
      "|Female|\n",
      "|  Male|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('sex').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Null values\n",
    "\n",
    "Columns can take the special value `null`, which is **never equal to any other value** (including itself), and uncomparable with other values (i.e., any relational operator returns `False`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are two ways of dealing with `null` values:\n",
    "\n",
    "1. Use `.isNull()` and `.isNotNull()` to select rows with `null` is a `.where()` clause.\n",
    "2. Use the `.fillna(val)` method to replace any occurrence of `null` with `val`; an optional list of columns where to apply the replacement can follow: \n",
    "\n",
    "```\n",
    "     # only act on columns `c1` and `c2`\n",
    "     df2 = df.fillna(val, ['c1', 'c2'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grouping data and aggregate computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us tackle the following problem: *compute how many words are uttered by children of a given age.*\n",
    "\n",
    "Since each subject has many table entries (one per uttered word), then we need to:\n",
    "\n",
    "* first, aggregate rows based on subject (`data_id`) **and** age, \n",
    "* then, compute the number of words per subject.  \n",
    "* after, we can further aggregate on age alone and sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The [`.groupby()`]( https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.Column) method of `DataFrame`'s creates an intermediate object that is *like* a table with aggregate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = df.groupby(['age', 'data_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.group.GroupedData"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** as usual, one can use arbitrary column expressions, not just column names, in a `.groupBy()` argument list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The only methods that we can call on a \"groupby\" object are those that apply a summarization function to the row groups.  \n",
    "\n",
    "The available summarization functions are defined in module [`pyspark.sql.functions`]( https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.Column) (which must be imported!) and are called \"aggregate functions\" in the documentation.\n",
    "\n",
    "Functions [`count()`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.functions.count),\n",
    "[`mean()`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.functions.mean),\n",
    "[`max()`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.functions.max),\n",
    "[`min()`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.functions.min),\n",
    "[`sum()`](https://spark.apache.org/docs/2.2.1/api/python/pyspark.sql.html#pyspark.sql.functions.sum) are of course present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+\n",
      "|age|data_id|nr_words|\n",
      "+---+-------+--------+\n",
      "| 28|  52509|      37|\n",
      "| 28|  52514|      43|\n",
      "| 24|  52548|      40|\n",
      "+---+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df3 = gd.agg(F.sum(df.produces.cast('integer')).alias('nr_words'))\n",
    "\n",
    "# show it\n",
    "df3.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the resulting DF has the aggregation columns *and* any column computed by `.agg()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 3.D\n",
    "\n",
    "Compute the total number of words uttered relative to age, for: all children, male children, female children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|nr_words|\n",
      "+---+--------+\n",
      "| 28|   28143|\n",
      "| 27|    5740|\n",
      "| 26|    5011|\n",
      "| 22|    4133|\n",
      "| 16|    6032|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. aggregate\n",
    "gd = df.groupBy('age')\n",
    "\n",
    "# 2. sum over 'produces' column\n",
    "df_ = gd.agg(F.sum(df.produces.cast('integer')).alias('nr_words'))\n",
    "\n",
    "\n",
    "# 3. show\n",
    "df_.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+\n",
      "|age| sex|nr_words|\n",
      "+---+----+--------+\n",
      "| 26|Male|    1773|\n",
      "| 25|Male|    2571|\n",
      "| 24|Male|    4843|\n",
      "| 27|Male|    2032|\n",
      "| 20|Male|    1296|\n",
      "+---+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. aggregate\n",
    "gd2 = df.groupBy(['age', 'sex'])\n",
    "\n",
    "# 2. sum over 'produces' column\n",
    "df_mf = gd2.agg(F.sum(df.produces.cast('integer')).alias('nr_words'))\n",
    "\n",
    "\n",
    "# 3. show\n",
    "df_mf.where(df_mf.sex == 'Male').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.E\n",
    "\n",
    "Make a bar plot of the number of words produced by age.\n",
    "\n",
    "(*Hint:* Seaborn provides an easy-to-use `.barplot` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbeb0caa3d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x=\"age\", y=\"nr_words\", data=df_.toPandas(), label=\"Nr. of distinct animal names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A lengthier solution is to pull the data out using `df_.collect()` then separate the `x` and `y` arrays for the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbeb0caac50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows= df_.collect()\n",
    "\n",
    "xs = [row[0] for row in rows]\n",
    "ys = [row[1] for row in rows]\n",
    "\n",
    "sns.barplot(x=xs, y=ys)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2 with Spark",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
